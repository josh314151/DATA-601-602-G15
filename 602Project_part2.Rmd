---
title: "602Project_part2"
output: pdf_document
date: "2024-02-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(dplyr)
library(mosaic)
library(resampledata)
library(olsrr)
```

Part1: ***Read data***:

```{r}
# Read data 
data_s1 = read.csv("summary_data.csv")
data_stages = read.csv("sleep_stage_output.csv")

data = c(data_s1, data_stages)

data = data.frame(data)

data = filter(data, Age>0)
data = filter(data, W < 400)
```

***Build Model***:

```{r}
ggplot(data, aes(x = Age, y = W)) + geom_smooth(method = "lm") + geom_point() + xlab("Age") + ylab("Wake Time") + ggtitle("Age vs Wake Time") 
predicted_rate = lm(W ~ Age, data=data)
predicted_rate$coef
```

The linear regression equation representing the model is $\hat{HR_{i}} = 78.4163472 - 0.1852872*Age_{i}$. ***Correlation Coefficient Check***:

```{r}
age <- data$Age
hr <- data$W
cor(age, hr)
```

Correlation Coefficient shows that there is a negative correlation between the age and average HR during sleep.

***Check significance of coefficient estimates*** Null hypothesis: $H_{0}: \beta_{1} = 0$, $H_{0}: \beta_{0} = 0$

Alternative hypothesis: $H_{A}: \beta_{1} \neq 0$, $H_{0}: \beta_{0} \neq 0$

We will set the alpha value to 0.05.

We can use a t test to check our claim.

```{r}
summary(predicted_rate)
```

From our t-test, we get test statistics of $\beta_{0}$ is 21.319 and $\beta_{1}$ is -2.626. P-values of $\beta_{0}$ is $2*10^-16$ and $\beta_{1}$ is 0.01041. The p-values are smaller than the set alpha value of 0.05, so we reject our null hypothesis that the linear regression coefficient and intercept are 0. Therefore, we can conclude that the average HR during sleep can be expressed as a linear function of age, and since $\beta_{1} < 0$, we can say it is also negative.

We also get an R-squared value of 0.08221, meaning our independent variable explains approx. 8.22% of the variance in the dependent variable.

***Residual Analysis***:

There are two conditions that must be met for our linear regression model to be valid.

**1. Normality of residuals:** The dependent variable (hrat) must be normally distributed with a mean of $\mu$ and standard deviations of $\sigma$. To check this we will plot a stat_qq plot of the residuals since $e_{i} = y_{i} - \hat{y_{i}}$, if y is normally distributed, so will the residuals.

**2. Homoscedasticity:** For each distinct value of the independant variable (season), the dependent variable (hrat) has the same standard deviation $\sigma$. To check this, we will plot a scatterplot of the fitted values and the residuals.

```{r}
# Get the and residuals fitted values
predicted.rate = predicted_rate$fitted.values
ei_hrat = predicted_rate$residuals  
data.df = data.frame(predicted.rate, ei_hrat)
```

**Normality of Residuals Plot:**

```{r}
ggplot(data.df, aes(sample = ei_hrat)) +  stat_qq(col='blue') + stat_qqline(col='red') + ggtitle("Normal Probability Plot of Residuals")
```

Looking the normality probability plot, the residuals do seem to be approximately normally distributed and therefore so is the dependent variable (hrat). The normality of residuals condition holds.

**Homoscedasticity:**

```{r}
ggplot(data.df, aes(x = predicted.rate, y = ei_hrat)) +  geom_point(size=2, col='blue', position="jitter") + xlab("Predicted Home Run Rate") + ylab("Residuals") + ggtitle("Plot of Fits to Residuals") + geom_hline(yintercept=0, color="red", linetype="dashed")
```

Looking the plot of fits to residuals, the residuals do seem to be evenly distributed over the home run rate. We can they say that the condition of homoscedasticity holds.

Since both conditions hold, our linear regression model is valid.

```{r}
ols_test_breusch_pagan(predicted_rate)
```

```{r}
shapiro.test(data$Age)
shapiro.test(data$W)
```
